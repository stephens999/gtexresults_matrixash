{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Multivariate adaptive shrinkage (MASH) analysis of GTEx data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This notebook contains code used to generate results for the Urbut *et al* (2017) manuscript.\n",
    "\n",
    "Important notes:\n",
    "\n",
    "1. Although you may open this notebook in Jupyter, you should not step through the code sequentially as you would in a typical Jupyter notebook; this is because the code in this notebook is meant to be run using the [Script of Scripts (SoS)](https://github.com/vatlab/SoS)\n",
    "framework.\n",
    "\n",
    "2. This notebook is meant to reproduce Urbut 2017, even though with the `--data` option you can provide your own data-set and perform MASH analysis, we recommand using [a more recent version of MASH implementation](https://github.com/stephenslab/mashr) for your analysis, which is a lot faster due to improvements in both algorithm and coding.\n",
    "\n",
    "3. In this notebook, we did not apply the inference to all the gene-snp pairs. Rather we focused on the \"top\" gene-snp pairs as a demonstration. It should be straightforward to configure the Posterior computatoin step to work on all gene-snp pairs instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Run MASH as implemented in Urbut 2017\n",
    "For repeated runs it might be easier to execute from commandline instead of in Jupyter:\n",
    "\n",
    "```bash\n",
    "sos run workflows/gtex6_mash_analysis.ipynb # --data ... --cwd ...\n",
    "```\n",
    "\n",
    "The notebook runs default setting, ie, all the analysis steps. To view all available analysis:\n",
    "\n",
    "```bash\n",
    "sos run workflows/gtex6_mash_analysis.ipynb -h\n",
    "```\n",
    "\n",
    "Additionally I run it for dataset after LD pruning (for LD related discussion in supplemental information):\n",
    "\n",
    "```bash\n",
    "sos run workflows/gtex6_mash_analysis.ipynb --data data/MatrixEQTLSumStats.Portable.ld2.Z.rds\n",
    "```\n",
    "\n",
    "The outcome of this notebook should be found under `./output` folder (can be configured), with the following output:\n",
    "\n",
    "```\n",
    "gtex6_mash_analysis.html\n",
    "MatrixEQTLSumStats.Portable.Z.sfa.rds.log\n",
    "MatrixEQTLSumStats.Portable.Z.sfa.rds\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.rds\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.lite.single.expanded.rds\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.lite.single.expanded.pihat.rds\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.lite.single.expanded.loglik.rds\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.lite.single.expanded.posterior.rds\n",
    "```\n",
    "\n",
    "We keep track of results from every MASH step, though the inference of interest should be found in the `*.posterior.rds` file generated at the end of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Analysis Steps\n",
    "\n",
    "The pipeline automatically executes the following:\n",
    "\n",
    "+ Compute a sparse factorization of the (centered) z-scores using the\n",
    "  [SFA software](http://stephenslab.uchicago.edu/software.html#sfa),\n",
    "  with K = 5 factors, and save the factors in an `.rds` file. This\n",
    "  will be used to construct the mixture-of-multivariate normals\n",
    "  prior. This step is labeled `sfa`, and should only take a few\n",
    "  minutes to run.\n",
    "\n",
    "+ Compute additional \"data-driven\" prior matrices by computing a\n",
    "  singular value decomposition of the (centered) z-scores and low-rank\n",
    "  approximations to the empirical covariance matrices. Most of the\n",
    "  work in this step involves running the Extreme Deconvolution\n",
    "  method. The outcome of running the Extreme Deconvolution method is\n",
    "  saved to a new `.rds` file. This step is labeled `mash-paper_1` and\n",
    "  may take several hours to run (in one run on a MacBook Pro with\n",
    "  a 3.5 GHz Intel Core i7, it took over 6 hours to complete).\n",
    "\n",
    "+ A final collection of \"canonical\" and single-rank prior matrices\n",
    "  based on SFA and the \"BMAlite\" models of Flutre *et al*\n",
    "  (2013). These matrices are again written to another `.rds` file. This\n",
    "  step is labeled `mash-paper_2`, and should take at most a minute to\n",
    "  run.\n",
    "\n",
    "+ The `mash-paper_3` step fits the MASH (\"multivariate adaptive\n",
    "  shrinkage\") model to the GTEx data (the centered z-scores); the\n",
    "  model parameters estimated in this fitting step are the weights of\n",
    "  the multivariate normal mixture. The outputs from this step are the\n",
    "  estimated mixture weights and the conditional likelihood\n",
    "  matrix. These two outputs are saved to two separate `.rds` files.\n",
    "  This step is expected to take at most a few hours to complete.\n",
    "\n",
    "+ The `mash-paper_4` step computes posterior statistics using the\n",
    "  fitted MASH model from the previous step. These posterior statistics\n",
    "  are summarized and visualized in subsequent analyses. The posterior\n",
    "  statistics are saved to another `.rds` file. This step is expected\n",
    "  to take a few hours to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### The prototype (default) MASH implementation: some details\n",
    "\n",
    "Some notes about the code used for the `mash` workflow analysis (some available from the `mash` workflow, some in the downloaded `mash_script_download` code):\n",
    "    \n",
    "    1. `deconvolution.em.with.bovy(...)`\n",
    "\n",
    "produces an object with the denoised matrices for feeding into the\n",
    "*mash* covariance code. The *factor.mat* and *lambda.mat* called\n",
    "within have been produced by SFA and are single rank factors and\n",
    "loadings approximating the empirical covariance.\n",
    "\n",
    "    2. `compute.hm.covmat.all.max.step(...)$covmat` \n",
    "\n",
    "produces a list of covariance matrices entitled *covmat\"A\".rds* upon\n",
    "which to base the mixture of multivariate normals.\n",
    "\n",
    "    3. `compute.hm.train.log.lik(...)`\n",
    "\n",
    "computes the HM weights on training datauses the set of randomly chosen genes to train our model and produces\n",
    "a matrix of likelihoods and corresponding hierarchical weights, as well as the mixture proportions.\n",
    "\n",
    "    4. `total.quant.per.snp(...)`\n",
    "\n",
    "produces files containing the posterior means, upper and lower\n",
    "tail probabilities, null probabilites, and lfsr for all J genes across\n",
    "44 conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path('./output')\n",
    "parameter: data = path(\"data/MatrixEQTLSumStats.Portable.Z.rds\")\n",
    "# path configured to /opt folder outside $HOME, to make it easier to use with `docker`\n",
    "parameter: mash_src = file_target(\"/opt/mash-paper/main.R\")\n",
    "parameter: sfa_exe = file_target(\"/opt/sfa/bin/sfa_linux\")\n",
    "parameter: vhat = 1\n",
    "sfa_data = file_target(f\"{cwd:a}/{data:bn}.sfa.rds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Covariance pattern discovery\n",
    "This obtains covariance matrices, ie, the priors, for `mash` model.\n",
    "\n",
    "### SFA\n",
    "We analyze data with SFA. The cell below downloads SFA software and run it on data with rank `K = 5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[sfa_download: provides = sfa_exe]\n",
    "# Download / install SFA (no need if running from docker `gaow/mash-paper`)\n",
    "download: decompress = True, dest_dir = f'{sfa_exe:ad}'\n",
    "    http://stephenslab.uchicago.edu/assets/software/sfa/sfa1.0.tar.gz\n",
    "\n",
    "[sfa: provides = sfa_data]\n",
    "# Perform SFA analysis (time estimate: <1min)\n",
    "depends: sfa_exe\n",
    "K = 5\n",
    "tmpfile = path(f\"{cwd:a}/{data:bn}.max.txt\")\n",
    "input: f\"{data:a}\"\n",
    "output: sfa_data\n",
    "R: expand = \"${ }\", stdout = f\"{_output:n}.log\", workdir = cwd\n",
    "    z = readRDS(${_input:r})$strong.z\n",
    "    write.table(z, ${tmpfile:r}, col.names=F,row.names=F)\n",
    "    cmd = paste0('${sfa_exe} -gen ${tmpfile} -g ', dim(z)[1], ' -n ', dim(z)[2], \n",
    "                 ' -k ${K} -iter 50 -rand 999 -o ${_output:bn}')\n",
    "    system(cmd)\n",
    "    saveRDS(list(F = read.table(\"${_output:n}_F.out\"),\n",
    "                lambda = read.table(\"${_output:n}_lambda.out\"),\n",
    "                sigma2 = read.table(\"${_output:n}_sigma2.out\"),\n",
    "                alpha = read.table(\"${_output:n}_alpha.out\")), ${_output:r})\n",
    "bash: workdir = cwd, expand = '${ }'\n",
    "    rm -f *{_F.out,_lambda.out,_sigma2.out,_alpha.out} ${tmpfile}\n",
    "    rm -r output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "###  Create and refine multi-rank covariance matrices\n",
    "Here we create 3 covariance matrices:\n",
    "\n",
    "* SFA (rank 5, previously computed)\n",
    "* SVD (rank 3, to be computed)\n",
    "* Empirical covariance\n",
    "\n",
    "and apply [Extreme Deconvolution](https://github.com/jobovy/extreme-deconvolution) to refine the matrices. We observed that Extreme Deconvolution perserves rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mash_scripts_download: provides = mash_src]\n",
    "# Download / install MASH scripts (no need if running from docker `gaow/mash-paper`)\n",
    "output: mash_src\n",
    "download: decompress = True, dest_dir = cwd\n",
    "    https://github.com/stephenslab/mashr-paper/archive/v0.2-1.zip\n",
    "bash: expand = True, workdir = cwd\n",
    "    mkdir -p {mash_src:ad}\n",
    "    cp mashr-paper-0.2-1/R/* {mash_src:ad} && rm -rf mashr-paper-0.2-1\n",
    "\n",
    "[mash-paper_1: shared = {'mash_input': '_input'}]\n",
    "# Compute data-driven prior matrices \n",
    "# (time estimate: 40min to 4hrs depending on the machine power)\n",
    "depends: R_library(\"mashr\"), mash_src, sfa_data\n",
    "K = 3\n",
    "P = 3\n",
    "input: f\"{data:a}\"\n",
    "output: f\"{cwd:a}/{data:bn}.coved.K{K}.P{P}.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    library(mashr) # `mashr` package offers function `extreme_deconvolution`\n",
    "    script.source = sapply(list.files(${mash_src:dar}, pattern = \"*.R\", full.names=TRUE), source, .GlobalEnv)\n",
    "    z.stat = readRDS(${_input:r})$strong.z\n",
    "    sfa = readRDS(${sfa_data:r})\n",
    "    s.j = matrix(rep(1,ncol(z.stat)*nrow(z.stat)),ncol=ncol(z.stat),nrow=nrow(z.stat))\n",
    "    res = deconvolution.em.with.bovy(z.stat, \n",
    "                                      as.matrix(sfa$F), \n",
    "                                      s.j, \n",
    "                                      as.matrix(sfa$lambda),\n",
    "                                      K = ${K}, P = ${P})\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Add in canonical and single-rank covariance matrices\n",
    "\n",
    "Now additionally we include 2 other types of covariance matrices:\n",
    "* canonical configurations (aka `bmalite`)\n",
    "* single rank SFA\n",
    "\n",
    "We also expand the list of matrices by grid. At the end of this step (cell below) we are ready to fit the mash model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mash-paper_2: shared = {'prior_matrices': '_output'}]\n",
    "# Add in canonical configurations and single rank SFA priors (time estimate: <1min)\n",
    "depends: sos_variable('mash_input'), sfa_data\n",
    "output: f\"{_input:n}.lite.single.expanded.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    script.source = sapply(list.files(${mash_src:dar}, pattern = \"*.R\", full.names=TRUE), source, .GlobalEnv)\n",
    "    z.stat = readRDS(${mash_input:r})$strong.z\n",
    "    rownames(z.stat) = NULL\n",
    "    colnames(z.stat) = NULL\n",
    "    s.j = matrix(rep(1,ncol(z.stat)*nrow(z.stat)),ncol=ncol(z.stat),nrow=nrow(z.stat))\n",
    "    sfa = readRDS(${sfa_data:r})\n",
    "    res = compute.hm.covmat.all.max.step(b.hat=z.stat,se.hat=s.j,\n",
    "                                          t.stat=z.stat,Q=5,\n",
    "                                          lambda.mat=as.matrix(sfa$lambda),\n",
    "                                          A='.${_input:bn}.remove_before_rerun',\n",
    "                                          factor.mat=as.matrix(sfa$F),\n",
    "                                          max.step=readRDS(${_input:r}),\n",
    "                                          zero=TRUE)\n",
    "    saveRDS(res$covmat, ${_output:r})\n",
    "\n",
    "bash: workdir = cwd, expand = True\n",
    "    rm -f *.{_input:bn}.remove_before_rerun.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Fit MASH mixture model\n",
    "Using the random SNP set, the cell below computes the weights for input covariance matrices (priors) in MASH mixture. The output contains matrix of log-likelihoods as well as weights learned from the hierarchical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mash-paper_3]\n",
    "# Fit MASH mixture model (time estimate: ~2.5hr)\n",
    "depends: sos_variable('mash_input'), R_library(\"SQUAREM\")\n",
    "output: f\"{_input:n}.V{vhat}.pihat.rds\", f\"{_input:n}.V{vhat}.loglik.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    library(\"SQUAREM\")\n",
    "    script.source = sapply(list.files(${mash_src:dar}, pattern = \"*.R\", full.names=TRUE), source, .GlobalEnv)\n",
    "    dat = readRDS(${mash_input:r})\n",
    "    covmat = readRDS(${_input:r})\n",
    "    random.z = as.matrix(dat$random.z)\n",
    "    rownames(random.z) = NULL\n",
    "    colnames(random.z) = NULL\n",
    "    random.v = matrix(rep(1,ncol(random.z)*nrow(random.z)),ncol=ncol(random.z),nrow=nrow(random.z))\n",
    "    res = compute.hm.train.log.lik.pen.vmat(train.b=random.z,\n",
    "                                            covmat=covmat,\n",
    "                                            cormat=${\"dat$vhat\" if vhat else \"diag(nrow(dat$vhat))\"},\n",
    "                                            A='.${_output[0]:bnn}.remove_before_rerun', \n",
    "                                            pen=TRUE,\n",
    "                                            train.s=random.v)\n",
    "    saveRDS(res$pis, ${_output[0]:r})\n",
    "    saveRDS(res$lik.mat, ${_output[1]:r})\n",
    "\n",
    "bash: workdir = cwd, expand = True\n",
    "    rm -f *.{_output[0]:bnn}.remove_before_rerun.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Posterior inference\n",
    "Applying hyperparameters learned from the random set to the top set (strongest cis SNP of eQTL association), the cell below computes posterior quantities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mash-paper_4]\n",
    "# Posterior inference on the \"top\" set of gene-snp pairs \n",
    "# (time estimate: ~5hr on single thread)\n",
    "depends: sos_variable('mash_input'), sos_variable('prior_matrices')\n",
    "output: f\"{_input[0]:nn}.posterior.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    script.source = sapply(list.files(${mash_src:dar}, pattern = \"*.R\", full.names=TRUE), source, .GlobalEnv)\n",
    "    dat = readRDS(${mash_input:r})\n",
    "    z.stat = dat$strong.z\n",
    "    s.j = matrix(rep(1,ncol(z.stat)*nrow(z.stat)),ncol=ncol(z.stat),nrow=nrow(z.stat))\n",
    "    pis = readRDS(${_input[0]:r})$pihat\n",
    "    covmat = readRDS(${prior_matrices:r})\n",
    "    res = lapply(seq(1:nrow(z.stat)), function(j){\n",
    "          total.quant.per.snp.with.vmat(j=j, \n",
    "                                      covmat=covmat, \n",
    "                                      b.gp.hat=z.stat,\n",
    "                                      se.gp.hat=s.j, \n",
    "                                      cormat=${\"dat$vhat\" if vhat else \"diag(nrow(dat$vhat))\"},\n",
    "                                      pis=pis, \n",
    "                                      A='', \n",
    "                                      checkpoint=TRUE)})\n",
    "    # data formatting.\n",
    "    out = do.call(Map, c(f = rbind, res))\n",
    "    saveRDS(out, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Now MASH analysis is complete. I will use separate vignettes to summarize, plot and visualize the result of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Export and run default pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[export]\n",
    "# Export notebook to HTML file\n",
    "input: [item for item in paths(sys.argv) if item.suffix == '.ipynb'], group_by = 1\n",
    "output: [(f'{cwd:a}/{item:bn}.full.html', f'{cwd:a}/{item:bn}.lite.html') for item in paths(sys.argv) if item.suffix == '.ipynb'], group_by = 2\n",
    "bash: expand = True, stderr = False\n",
    "  sos convert {_input} {_output[0]}\n",
    "  sos convert {_input} {_output[1]} --template sos-report-toc\n",
    "  \n",
    "[default]\n",
    "# Run all analysis in this notebook\n",
    "sos_run('export')\n",
    "sos_run('mash-paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p class=\"session_section\">SoS</p>\n",
       "<table class=\"session_info\">\n",
       "<tr>\n",
       "<th>SoS Version</th><td><pre>0.9.14.2</pre></td>\n",
       "</tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sessioninfo"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.18.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
